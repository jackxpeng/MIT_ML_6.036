{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIaEwCD406A"
   },
   "source": [
    "#MIT 6.036 Spring 2019: Homework 7#\n",
    "\n",
    "This colab notebook provides code and a framework for problem 2 of [the homework](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week7/week7_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
    "\n",
    "## <section>**Setup**</section>\n",
    "\n",
    "First, download the code distribution for this homework that contains test cases and helper functions.\n",
    "\n",
    "Run the next code block to download and import the code for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YM-_zLf9Bp-",
    "outputId": "c280d0b2-6978-45d0-fd69-cfc5e9083035"
   },
   "outputs": [],
   "source": [
    "from code_for_hw7 import *\n",
    "import numpy as np\n",
    "import modules_disp as disp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFxhrJ5XDlvb"
   },
   "source": [
    "# 2) Implementing Neural Networks\n",
    "\n",
    "This homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
    "\n",
    "<br>\n",
    "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_jz$\n",
    "\n",
    "specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n",
    "\n",
    "<br>\n",
    "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjQgtwPHj08n"
   },
   "source": [
    "We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
    "\n",
    "```\n",
    "# build a 3-layer network\n",
    "net = Sequential([Linear(2,3), Tanh(),\n",
    "                  Linear(3,3), Tanh(),\n",
    "    \t          Linear(3,2), SoftMax()])\n",
    "# train the network on data and labels\n",
    "net.sgd(X, Y)\n",
    "```\n",
    "Please fill in any unimplemented methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEwpgsbnho9K"
   },
   "source": [
    "## Linear Modules: ##\n",
    "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z.\n",
    "\n",
    "Each linear module has a backward method that takes in dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-VsYLAxCfy7U"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
    "        # return None  # Your code (n x b)\n",
    "        return self.W.T @ self.A + self.W0\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        # self.dLdW = None       # Your code\n",
    "        # self.dLdW0 = None      # Your code\n",
    "        # return None            # Your code: return dLdA (m x b)\n",
    "\n",
    "        # dLdW\n",
    "        self.dLdW = self.A @ dLdZ.T\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=-1, keepdims=True)\n",
    "        return self.W @ dLdZ\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        # self.W = None           # Your code\n",
    "        # self.W0 = None          # Your code\n",
    "        self.W = self.W - lrate * self.dLdW\n",
    "        self.W0 = self.W0 - lrate * self.dLdW0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqZ7_kZYr5s5"
   },
   "source": [
    " You are encouraged to make your own tests for each module. A unit test method and an example test case are given below for your reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aY3yePY0r4eA",
    "outputId": "e6e2de27-60d0-4100-9529-455387ccb481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_forward: OK\n",
      "linear_backward: OK\n",
      "linear_sgd_step_W: OK\n",
      "linear_sgd_step_W0: OK\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# data\n",
    "X, Y = super_simple_separable()\n",
    "\n",
    "# module\n",
    "linear_1 = Linear(2, 3)\n",
    "\n",
    "#hyperparameters\n",
    "lrate = 0.005\n",
    "\n",
    "# test case\n",
    "# forward\n",
    "z_1 = linear_1.forward(X)\n",
    "exp_z_1 =  np.array([[10.41750064, 6.91122168, 20.73366505, 22.8912344],\n",
    "                     [7.16872235, 3.48998746, 10.46996239, 9.9982611],\n",
    "                     [-2.07105455, 0.69413716, 2.08241149, 4.84966811]])\n",
    "unit_test(\"linear_forward\", exp_z_1, z_1)\n",
    "\n",
    "# backward\n",
    "dL_dz1 = np.array([[1.69467553e-09, -1.33530535e-06, 0.00000000e+00, -0.00000000e+00],\n",
    "                                     [-5.24547376e-07, 5.82459519e-04, -3.84805202e-10, 1.47943038e-09],\n",
    "                                     [-3.47063705e-02, 2.55611604e-01, -1.83538094e-02, 1.11838432e-04]])\n",
    "exp_dLdX = np.array([[-2.40194628e-02, 1.77064845e-01, -1.27021626e-02, 7.74006953e-05],\n",
    "                                    [2.39827939e-02, -1.75870737e-01, 1.26832126e-02, -7.72828555e-05]])\n",
    "dLdX = linear_1.backward(dL_dz1)\n",
    "unit_test(\"linear_backward\", exp_dLdX, dLdX)\n",
    "\n",
    "# sgd step\n",
    "linear_1.sgd_step(lrate)\n",
    "exp_linear_1_W = np.array([[1.2473734,  0.28294514,  0.68940437],\n",
    "                           [1.58455079, 1.32055711, -0.69218045]]),\n",
    "unit_test(\"linear_sgd_step_W\",  exp_linear_1_W,  linear_1.W)\n",
    "\n",
    "exp_linear_1_W0 = np.array([[6.66805339e-09],\n",
    "                            [-2.90968033e-06],\n",
    "                            [-1.01331631e-03]]),\n",
    "unit_test(\"linear_sgd_step_W0\", exp_linear_1_W0, linear_1.W0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ETL01mPsBz4"
   },
   "source": [
    "The following datasets are defined for your use:\n",
    "*  `super_simple_separable_through_origin()`\n",
    "*  `super_simple_separable()`\n",
    "*  `xor()`\n",
    "*  `xor_more()`\n",
    "*  `hard()`\n",
    "\n",
    "Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n",
    "```\n",
    "def plot_nn(X, Y, nn):\n",
    "    \"\"\" Plot output of nn vs. data \"\"\"\n",
    "    def predict(x):\n",
    "        return nn.modules[-1].class_fun(nn.forward(x))[0]\n",
    "    xmin, ymin = np.min(X, axis=1)-1\n",
    "    xmax, ymax = np.max(X, axis=1)+1\n",
    "    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n",
    "    plot_data(X, Y, nax)\n",
    "    plt.show()```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s70beWJh09h"
   },
   "source": [
    "## Activation functions: ##\n",
    "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
    "\n",
    "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwaNAtLnhenT"
   },
   "source": [
    "### Tanh: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ff6eD3dnftiR"
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        # return None              # Your code: return dLdZ (?, b)\n",
    "        return dLdA * (1 - self.A**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FW7ocKRhcgY"
   },
   "source": [
    "### ReLU: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1fm2KsLUfqdp"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        # self.A = None            # Your code: (?, b)\n",
    "        self.A = np.where(Z > 0, Z, 0)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        # return None              # Your code: return dLdZ (?, b)\n",
    "        return dLdA * np.where(self.A > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKtXuTQ0hSNO"
   },
   "source": [
    "###SoftMax: ###\n",
    "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fqK-CJrnfn22"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        # return None              # Your code: (?, b)\n",
    "        e = np.exp(Z)\n",
    "        s = np.sum(e, axis=0)\n",
    "        return e / s\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        # return None              # Your code: (1, b)\n",
    "        return np.argmax(Ypred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZc7HnMSh4fn"
   },
   "source": [
    "## Loss Functions:##\n",
    "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n",
    "\n",
    "The NLL module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with NLL loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4uy0pHVhNd8"
   },
   "source": [
    "### NLL: ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTxABZJnzvLM"
   },
   "source": [
    "You should use multi-class NLL.\n",
    "\n",
    "$$\n",
    "\\frac{dNLL(Softmax(z))}{dz} = Y_{pred} - Y  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "17Fb8mimflgb"
   },
   "outputs": [],
   "source": [
    "class NLL(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        # return None      # Your code\n",
    "        return np.sum(-Y * np.log(Ypred))\n",
    "\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        # return None      # Your code\n",
    "        return self.Ypred - self.Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1EffzDFkqMX"
   },
   "source": [
    "## Activation and Loss Test Cases: ##\n",
    "Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DJFzpahkvcD",
    "outputId": "5ff61ff7-400a-4314-b53e-f0fab537a774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ],
   "source": [
    "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bd0dXg-Qk05_",
    "outputId": "3f786d1a-88aa-4179-991f-82fb04275ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ],
   "source": [
    "# TEST 2: sgd_test for ReLU activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2,3), ReLU(), Linear(3,2), SoftMax()], NLL()), test_2_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l5JgBU2iBCZ"
   },
   "source": [
    "## Neural Network: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXMGcdnXgiF3"
   },
   "source": [
    "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
    "\n",
    "We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ejO15Vr7fhKB"
   },
   "outputs": [],
   "source": [
    " class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        loss = 0\n",
    "        for it in range(iters):\n",
    "            # pass                                  # Your code\n",
    "            i = np.random.randint(N)\n",
    "            Xt, Yt = X[:,i:i+1], Y[:,i:i+1]\n",
    "            Ypred = self.forward(Xt)\n",
    "            loss = self.loss.forward(Ypred, Yt)\n",
    "            dLdZ = self.loss.backward()\n",
    "            self.backward(dLdZ)\n",
    "            self.sgd_step(lrate)\n",
    "            self.print_accuracy(it, X, Y, loss, every=1000)            \n",
    "\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints current loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUojaXqphDjh"
   },
   "source": [
    "## Neural Network / SGD Test Cases: ##\n",
    "Use Test 3 and Test 4 to help you debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "wmupM8OScodw",
    "outputId": "b172b7a7-96c9-4079-e844-ab51ae801cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.5 \tLoss = 0.5581706560965126\n",
      "Iteration = 1001 \tAcc = 0.9 \tLoss = 0.031586160350111116\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 0.00016659134553862913\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 0.020286123191513204\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 0.15438350949516244\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 9.68985843104694e-05\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 6.260447309563392e-07\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 0.24577742711063694\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 0.10182137754030812\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 6.705385593349474e-07\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 0.05677179041228267\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 0.0009548453838319272\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 0.06998328757525492\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 1.3534248761398322e-05\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 0.04790784897278654\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 0.0452815187579465\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 0.2784948739132527\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 1.6414757930260906\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 4.754224802926788e-09\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 0.05351870746165137\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 0.00026224912652790294\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 2.2487014902723773e-07\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 0.030302114076007962\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 0.026300709990373785\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 1.1319186874139037e-08\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 5.609241646110788e-05\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 3.204651642327901e-05\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 0.008247562239567756\n",
      "Iteration = 28001 \tAcc = 0.95 \tLoss = 1.0096001812852694e-10\n",
      "Iteration = 29001 \tAcc = 0.95 \tLoss = 3.0423444306557834e-07\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 5.861955227486692e-05\n",
      "Iteration = 31001 \tAcc = 1.0 \tLoss = 8.629783171767433e-06\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 6.588064409059998e-06\n",
      "Iteration = 33001 \tAcc = 1.0 \tLoss = 0.7486804848204295\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = 3.7362075996119424e-06\n",
      "Iteration = 35001 \tAcc = 0.95 \tLoss = 4.685362066730477e-06\n",
      "Iteration = 36001 \tAcc = 0.95 \tLoss = 0.46754799080464315\n",
      "Iteration = 37001 \tAcc = 0.95 \tLoss = 8.034983585252469e-06\n",
      "Iteration = 38001 \tAcc = 1.0 \tLoss = 2.040782211971299e-09\n",
      "Iteration = 39001 \tAcc = 1.0 \tLoss = 0.01717301609081629\n",
      "Iteration = 40001 \tAcc = 1.0 \tLoss = 3.2951588179064747e-09\n",
      "Iteration = 41001 \tAcc = 0.95 \tLoss = 0.003056685232241097\n",
      "Iteration = 42001 \tAcc = 0.95 \tLoss = 0.02224519639194992\n",
      "Iteration = 43001 \tAcc = 0.95 \tLoss = 0.019605522228856285\n",
      "Iteration = 44001 \tAcc = 0.95 \tLoss = 1.9023303314701723e-06\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 0.010048205385177691\n",
      "Iteration = 46001 \tAcc = 0.95 \tLoss = 9.483167774110909e-06\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 6.483986743529273e-07\n",
      "Iteration = 48001 \tAcc = 0.95 \tLoss = 0.19042497471955888\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 2.388508930561093e-07\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 2.0732972223380448e-07\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 1.9928427816712997e-09\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 0.0014670574801216919\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 0.012472328020762773\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 0.2791636632932357\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 3.463221717356228e-07\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = 5.140424683724362e-07\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 0.3675922040346343\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 1.0280639757267924e-07\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 3.5787386451690294e-09\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 3.922437049529458e-09\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 0.004254495691995706\n",
      "Iteration = 62001 \tAcc = 0.95 \tLoss = 0.00019452816161731937\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 1.3954525664972517e-07\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 2.0649540677764752e-07\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.0006940295030955738\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 1.0551210247475103e-06\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 0.00041373869185732347\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 0.009593331872292696\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 1.051335685231279e-10\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 2.4955667475551205e-06\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 8.257512866646176e-07\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = 1.343336784173074e-08\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 1.4637791151365553e-07\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 9.422895744557766e-07\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 9.580986066838546e-07\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 4.791691655725818e-07\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.0005706424223040088\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 0.005540381345511716\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 0.009647353963579167\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 1.010366235172338e-10\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 0.0006953433481427615\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 2.793539763351367e-07\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 0.005464603127097132\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 2.1046656193634147e-06\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.00020526284952444813\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 5.307922929904672e-07\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 4.754092985810765e-07\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 6.254752271868325e-11\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 1.3735569358016293e-09\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 5.226159259859331e-07\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 0.0006126937635799547\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 1.9512789977961042e-07\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 0.05263785881356597\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 0.005273788968670497\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.0004169073511642133\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 0.0047139270015051265\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 4.238454032280298e-11\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 3.8773421607421733e-07\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 3.614173636842408e-08\n",
      "-3.46493986 -3.41956036 3.39710997 2.0597278\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x7fe54ee7fda0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST 3: you should achieve 100% accuracy on the hard dataset (note that we provided plotting code)\n",
    "X, Y = hard()\n",
    "nn = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10,2), SoftMax()], NLL())\n",
    "disp.classify(X, Y, nn, it=100001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MaWfgC7Qe3ar"
   },
   "outputs": [],
   "source": [
    "# TEST 4: try calling these methods that train with a simple dataset\n",
    "def nn_tanh_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_relu_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_pred_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    Ypred = nn.forward(X)\n",
    "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dx-zM2y3R0z",
    "outputId": "eb42b2d4-2a3e-4723-8855-d1e521bc8341"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
       "  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
       "  [-8.47337764291184e-12, 2.6227368810847102e-09, 0.00017353185263155828]],\n",
       " [[0.544808855557535, -0.08366117689965663],\n",
       "  [-0.06331837550937104, 0.24078409926389266],\n",
       "  [0.08677202043839037, 0.8360167748667923],\n",
       "  [-0.0037249480614718, 0.0037249480614718]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_tanh_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
    "#   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
    "#   [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
    "#  [[0.544808855557535, -0.08366117689965663],\n",
    "#   [-0.06331837550937104, 0.24078409926389266],\n",
    "#   [0.08677202043839037, 0.8360167748667923],\n",
    "#   [-0.0037249480614718, 0.0037249480614718]]]\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmYT9IWk3TQL",
    "outputId": "0ced503b-2bd1-4f9f-c6c0-6891fbcabe79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
       "  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
       "  [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
       " [[0.501769700845158, -0.040622022187279644],\n",
       "  [-0.09260786974986723, 0.27007359350438886],\n",
       "  [0.08364438851530624, 0.8391444067898763],\n",
       "  [-0.004252310922204504, 0.004252310922204505]]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_relu_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
    "#   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
    "#   [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
    "#  [[0.501769700845158, -0.040622022187279644],\n",
    "#   [-0.09260786974986723, 0.27007359350438886],\n",
    "#   [0.08364438851530624, 0.8391444067898763],\n",
    "#   [-0.004252310922204504, 0.004252310922204505]]]\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uo_woDFh3a2v",
    "outputId": "06fe9f35-cd37-415c-c8f9-e9975a6db5c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0], [np.float64(8.565750618357672)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pred_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# ([0, 0, 0, 0], [8.56575061835767])\n",
    "# '''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
